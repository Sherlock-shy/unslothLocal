{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "max_seq_length = 2048\n",
    "\n",
    "# Explicitly set dtype based on GPU capabilities\n",
    "if torch.cuda.is_available():\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    dtype = torch.bfloat16 if gpu_properties.major >= 8 else torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "# Path to your locally saved model\n",
    "local_model_path = \"/mnt/d/Yui/models/llama2Chat7b\"\n",
    "\n",
    "# Step 1: Load and modernize the tokenizer\n",
    "print(\"Loading and modernizing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "tokenizer.save_pretrained(local_model_path)\n",
    "print(f\"Tokenizer saved in modern format at: {local_model_path}\")\n",
    "\n",
    "# Step 2: Load the model\n",
    "print(\"Loading model with Unsloth...\")\n",
    "model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=local_model_path,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# Step 3: Adjust dropout probabilities\n",
    "print(\"Adjusting dropout probabilities...\")\n",
    "try:\n",
    "    model.config.hidden_dropout_prob = 0.1\n",
    "    model.config.attention_probs_dropout_prob = 0.1\n",
    "    print(f\"Dropout set: Hidden={model.config.hidden_dropout_prob}, Attention={model.config.attention_probs_dropout_prob}\")\n",
    "except AttributeError:\n",
    "    print(\"Dropout attributes not found in model config. Check the model architecture for direct adjustments.\")\n",
    "\n",
    "# Step 4: Validate dtype\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Available Memory: 12878086144\n"
     ]
    }
   ],
   "source": [
    "# making sure cuda is working \n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Available Memory:\", torch.cuda.get_device_properties(0).total_memory if torch.cuda.is_available() else \"N/A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Define EOS token\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "SYSTEM_PROMPT = \"\"\" \"\"\" # add your own system prompt\n",
    "# Formatting function for multi-turn conversations\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for conversation in examples['turns']:\n",
    "        convo_text = SYSTEM_PROMPT\n",
    "        for i, turn in enumerate(conversation):  # Use enumerate to access index and turn\n",
    "            role = turn['role']\n",
    "            content = turn['content']\n",
    "            \n",
    "            if role != \"Yui\":  # Treat all non-Yui roles as input\n",
    "                convo_text += f\"\\nUser: {content}\"\n",
    "            else:  # Treat Yui as the response\n",
    "                convo_text += f\"\\nYui: {content}\"\n",
    "            \n",
    "            # Add EOS token after the last turn of the conversation\n",
    "            if i == len(conversation) - 1:  # Last turn in the conversation\n",
    "                convo_text += f\" {EOS_TOKEN}\"\n",
    "                \n",
    "        texts.append(convo_text.strip())\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Paths to the datasets\n",
    "train_file = \"path_to_your_datasets\" #training datasets\n",
    "val_file = \"path_to_your_datasets\" #validation datasets\n",
    "test_file = \"path_to_your_datasets\" #testing datasets\n",
    "\n",
    "# Load and format the split datasets\n",
    "train_dataset = load_dataset(\"json\", data_files={\"train\": train_file})[\"train\"]\n",
    "val_dataset = load_dataset(\"json\", data_files={\"validation\": val_file})[\"validation\"]\n",
    "test_dataset = load_dataset(\"json\", data_files={\"test\": test_file})[\"test\"]\n",
    "\n",
    "# Apply formatting function\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "val_dataset = val_dataset.map(formatting_prompts_func, batched=True)\n",
    "test_dataset = test_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Combine into a DatasetDict for easier access during training and evaluation\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Example to verify the output. making sure you got the right thing\n",
    "print(\"Example formatted training data:\")\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code snippet is setting up a training configuration for a model using the TRL (Text Representation Learning) library. Here's a breakdown of what the code is doing:\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from unsloth import is_bfloat16_supported\n",
    "from transformers import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-7, weight_decay=0.02)  # Very small learning rate\n",
    "scheduler = StepLR(optimizer, step_size=1000, gamma=0.1)  # Reduces LR every 1000 steps\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=128,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        fp16=not is_bfloat16_supported(),       # Use FP16 unless BF16 is available\n",
    "        bf16=is_bfloat16_supported(),          # Enable BF16 for Ampere GPUs if supported\n",
    "        fp16_full_eval=True,                   # Use FP16 for evaluation\n",
    "        per_device_eval_batch_size=4,          # Smaller eval batch size for memory stability\n",
    "        eval_accumulation_steps=8,             # Accumulate evaluation results over multiple batches\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=10,                         # Evaluate every 10 steps for close monitoring\n",
    "        per_device_train_batch_size=8,         # Smaller training batch size for memory stability\n",
    "        gradient_accumulation_steps=25,        # Accumulate gradients for a larger effective batch size\n",
    "        warmup_steps=500,                      # Gradual warmup for smooth adjustment\n",
    "        max_steps=5000,                        # Total number of training steps\n",
    "        learning_rate=2e-5,                    # Start with a conservative learning rate\n",
    "        logging_steps=10,                      # Log every 10 steps\n",
    "        optim=\"adamw_torch\",\n",
    "        weight_decay=0.02,                     # Moderate weight decay to reduce overfitting\n",
    "        lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "        lr_scheduler_kwargs={\"factor\": 0.5, \"patience\": 2, \"min_lr\": 1e-8}, \n",
    "        #lr_scheduler_kwargs={\"num_cycles\": 2},\n",
    "        #lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)\n",
    "        metric_for_best_model=\"eval_loss\",     # Track eval loss to determine best model\n",
    "        max_grad_norm=1.0,                     # Clip gradients for numerical stability\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        gradient_checkpointing=True,\n",
    "        save_steps=100,                         # Save frequently for better recovery\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    optimizers=(optimizer,scheduler),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]  # Stop early if no improvement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_cudnn_sdp(False)\n",
    "# Assign an existing token as the padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # You can use eos_token if it exists, or define a new one\n",
    "\n",
    "# Alternatively, add a new special padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))  # Resize embeddings to accommodate new token\n",
    "\n",
    "# Run training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer\n",
    "import torch\n",
    "\n",
    "# Create a data collator to pad sequences during evaluation\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=\"max_length\")\n",
    "\n",
    "# Define function to compute loss and ignore padding tokens in evaluation\n",
    "def compute_loss_ignore_padding(predictions, labels):\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    return loss_fct(predictions.view(-1, predictions.size(-1)), labels.view(-1))\n",
    "\n",
    "# Update the evaluation code to handle multi-turn conversations\n",
    "def format_and_tokenize_for_test(examples):\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    texts = []\n",
    "    for conversation in examples[\"turns\"]:\n",
    "        convo_text = \"\"\n",
    "        for turn in conversation:\n",
    "            role = turn[\"role\"]\n",
    "            content = turn[\"content\"]\n",
    "            if role != \"AI\":  # Treat all non-AI roles as user input\n",
    "                convo_text += f\"\\nUser: {content}\"\n",
    "            else:  # llm responses are treated as output\n",
    "                convo_text += f\"\\nAI: {content}\"\n",
    "        convo_text += EOS_TOKEN\n",
    "        texts.append(convo_text.strip())\n",
    "    \n",
    "    # Tokenize with truncation and padding for uniform input\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenization and formatting to the test dataset\n",
    "test_dataset = test_dataset.map(format_and_tokenize_for_test, batched=True)\n",
    "\n",
    "# Set format for PyTorch tensors and include padding\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# Ensure `trainer` is instantiated properly with the test dataset\n",
    "# Trainer should have been initialized elsewhere with model, tokenizer, etc.\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# Display test loss and other metrics\n",
    "print(\"Test Results:\", test_results)\n",
    "\n",
    "# Print specific test loss value if available\n",
    "test_loss = test_results.get(\"eval_loss\", None)\n",
    "if test_loss is not None:\n",
    "    print(f\"Test Loss (with padding handled): {test_loss:.4f}\")\n",
    "else:\n",
    "    print(\"No test loss found in the evaluation metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q5_k_m\", token = \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
